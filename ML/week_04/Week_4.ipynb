{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "p8oPV2Rm-jeM",
        "outputId": "1d4ede06-460d-4c6b-8cc0-d56770199c41"
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  \n",
        "y = iris.target\n",
        "Y = np.eye(3)[y]\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "def get_loss(y, a):\n",
        "  return -1 * np.sum(y * np.log(a))\n",
        "\n",
        "def get_loss_numerically_stable(y, z):\n",
        "   return -1 * np.sum(y * (z + (-z.max() - np.log(np.sum(np.exp(z-z.max()))))))\n",
        "\n",
        "def get_gradients(x, z, a, y):\n",
        "  da = (-y / a)\n",
        "\n",
        "  matrix = np.matmul(a, np.ones((1, 3))) * (np.identity(3) - np.matmul(np.ones((3, 1)), a.T))\n",
        "  dz = np.matmul(matrix, da)\n",
        "\n",
        "  dW = dz * x.T\n",
        "  db = dz.copy()\n",
        "\n",
        "  return dz, dW, db\n",
        "\n",
        "def gradient_descent(W, b, dW, db, learning_rate):\n",
        "  W = W - learning_rate * dW\n",
        "  b = b - learning_rate * db\n",
        "  return W, b\n",
        "\n",
        "def forward_propagate(x, W, b):\n",
        "  # W is 3 x 2\n",
        "  # x is 2 x 1\n",
        "  # b is 3 x 1\n",
        "  z = np.matmul(W, x) + b\n",
        "  a = stable_softmax(z)\n",
        "  # z is 3 x 1\n",
        "  # a is 3 x 1\n",
        "  return z, a\n",
        "\n",
        "def stable_softmax(z):\n",
        "  # z is 3 x 1\n",
        "  a = np.exp(z - max(z)) / np.sum(np.exp(z - max(z)))\n",
        "  # a is 3 x 1\n",
        "  return a\n",
        "\n",
        "\n",
        "# random initialization\n",
        "W_initial = np.random.rand(3, 2)\n",
        "W = W_initial.copy()\n",
        "b = np.zeros((3, 1))\n",
        "\n",
        "W_cache = []\n",
        "b_cache = []\n",
        "L_cache = []\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "  dW = np.zeros(W.shape)\n",
        "  db = np.zeros(b.shape)\n",
        "  L = 0\n",
        "  for j in range(X.shape[0]):\n",
        "    x_j = X[j,:].reshape(2,1)\n",
        "    y_j = Y[j,:].reshape(3,1)\n",
        "\n",
        "    z_j, a_j = forward_propagate(x_j, W, b)\n",
        "    loss_j = get_loss_numerically_stable(y_j, z_j)\n",
        "    dZ_j, dW_j, db_j = get_gradients(x_j, z_j, a_j, y_j)\n",
        "\n",
        "    dW += dW_j\n",
        "    db += db_j\n",
        "    L += loss_j\n",
        "\n",
        "  dW = (1/X.shape[0]) * dW # (1.0/17) * dw\n",
        "  db = (1/X.shape[0]) * db # (1.0/17) * db\n",
        "  \n",
        "  L = (1/X.shape[0]) * L # (1.0/17) * loss\n",
        "\n",
        "  W, b = gradient_descent(W, b, dW, db, LEARNING_RATE)\n",
        "\n",
        "  W_cache.append(W)\n",
        "  b_cache.append(b)\n",
        "  L_cache.append(L)\n",
        "\n",
        "plt.grid()\n",
        "plt.title('Loss', size=18)\n",
        "plt.xlabel('Number of iterations', size=15)\n",
        "plt.ylabel('Loss', size=15)\n",
        "plt.ylim([0, max(L_cache) * 1.1])\n",
        "plt.plot(L_cache)\n",
        "\n",
        "plt.savefig('image.png')\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slf1WeEbS7ET",
        "outputId": "8c916e09-c4f4-4e4a-b307-1bf6804b728d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "iris_data = load_iris() # load the iris dataset\n",
        "\n",
        "print('Example data: ')\n",
        "print(iris_data.data[:5])\n",
        "print('Example labels: ')\n",
        "print(iris_data.target[:5])\n",
        "\n",
        "x = iris_data.data[:, :2] \n",
        "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
        "\n",
        "\n",
        "# One Hot encode the class labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y_)\n",
        "print('all of y')\n",
        "print(y)\n",
        "\n",
        "# Split the data for training and testing\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n",
        "\n",
        "# Build the model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10, activation='relu', name='fc1'))  # input_shape=(4,)\n",
        "model.add(Dense(10, activation='relu', name='fc2'))\n",
        "model.add(Dense(3, activation='softmax', name='output'))\n",
        "\n",
        "# Adam optimizer with learning rate of 0.001\n",
        "optimizer = Adam(lr=0.001)\n",
        "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print('Neural Network Model Summary: ')\n",
        "#print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\n",
        "\n",
        "# Test on unseen data\n",
        "\n",
        "results = model.evaluate(test_x, test_y)\n",
        "\n",
        "print('Final test set loss: {:4f}'.format(results[0]))\n",
        "print('Final test set accuracy: {:4f}'.format(results[1]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example data: \n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "Example labels: \n",
            "[0 0 0 0 0]\n",
            "all of y\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n",
            "Neural Network Model Summary: \n",
            "Epoch 1/200\n",
            "24/24 - 0s - loss: 1.5871 - accuracy: 0.3500\n",
            "Epoch 2/200\n",
            "24/24 - 0s - loss: 1.2047 - accuracy: 0.3500\n",
            "Epoch 3/200\n",
            "24/24 - 0s - loss: 1.0887 - accuracy: 0.4167\n",
            "Epoch 4/200\n",
            "24/24 - 0s - loss: 1.0170 - accuracy: 0.5583\n",
            "Epoch 5/200\n",
            "24/24 - 0s - loss: 0.9905 - accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "24/24 - 0s - loss: 0.9611 - accuracy: 0.6167\n",
            "Epoch 7/200\n",
            "24/24 - 0s - loss: 0.9355 - accuracy: 0.6333\n",
            "Epoch 8/200\n",
            "24/24 - 0s - loss: 0.9150 - accuracy: 0.6167\n",
            "Epoch 9/200\n",
            "24/24 - 0s - loss: 0.8974 - accuracy: 0.6667\n",
            "Epoch 10/200\n",
            "24/24 - 0s - loss: 0.8810 - accuracy: 0.6500\n",
            "Epoch 11/200\n",
            "24/24 - 0s - loss: 0.8619 - accuracy: 0.6833\n",
            "Epoch 12/200\n",
            "24/24 - 0s - loss: 0.8478 - accuracy: 0.6750\n",
            "Epoch 13/200\n",
            "24/24 - 0s - loss: 0.8308 - accuracy: 0.6333\n",
            "Epoch 14/200\n",
            "24/24 - 0s - loss: 0.8155 - accuracy: 0.6833\n",
            "Epoch 15/200\n",
            "24/24 - 0s - loss: 0.8010 - accuracy: 0.6667\n",
            "Epoch 16/200\n",
            "24/24 - 0s - loss: 0.7847 - accuracy: 0.6500\n",
            "Epoch 17/200\n",
            "24/24 - 0s - loss: 0.7678 - accuracy: 0.6667\n",
            "Epoch 18/200\n",
            "24/24 - 0s - loss: 0.7520 - accuracy: 0.6750\n",
            "Epoch 19/200\n",
            "24/24 - 0s - loss: 0.7419 - accuracy: 0.6583\n",
            "Epoch 20/200\n",
            "24/24 - 0s - loss: 0.7300 - accuracy: 0.6833\n",
            "Epoch 21/200\n",
            "24/24 - 0s - loss: 0.7200 - accuracy: 0.6583\n",
            "Epoch 22/200\n",
            "24/24 - 0s - loss: 0.7006 - accuracy: 0.6750\n",
            "Epoch 23/200\n",
            "24/24 - 0s - loss: 0.6906 - accuracy: 0.6750\n",
            "Epoch 24/200\n",
            "24/24 - 0s - loss: 0.6847 - accuracy: 0.6833\n",
            "Epoch 25/200\n",
            "24/24 - 0s - loss: 0.6682 - accuracy: 0.6917\n",
            "Epoch 26/200\n",
            "24/24 - 0s - loss: 0.6648 - accuracy: 0.6833\n",
            "Epoch 27/200\n",
            "24/24 - 0s - loss: 0.6534 - accuracy: 0.6750\n",
            "Epoch 28/200\n",
            "24/24 - 0s - loss: 0.6453 - accuracy: 0.6917\n",
            "Epoch 29/200\n",
            "24/24 - 0s - loss: 0.6360 - accuracy: 0.6833\n",
            "Epoch 30/200\n",
            "24/24 - 0s - loss: 0.6298 - accuracy: 0.6917\n",
            "Epoch 31/200\n",
            "24/24 - 0s - loss: 0.6162 - accuracy: 0.6917\n",
            "Epoch 32/200\n",
            "24/24 - 0s - loss: 0.6144 - accuracy: 0.6667\n",
            "Epoch 33/200\n",
            "24/24 - 0s - loss: 0.6071 - accuracy: 0.6667\n",
            "Epoch 34/200\n",
            "24/24 - 0s - loss: 0.5946 - accuracy: 0.7000\n",
            "Epoch 35/200\n",
            "24/24 - 0s - loss: 0.5883 - accuracy: 0.7083\n",
            "Epoch 36/200\n",
            "24/24 - 0s - loss: 0.5819 - accuracy: 0.7000\n",
            "Epoch 37/200\n",
            "24/24 - 0s - loss: 0.5773 - accuracy: 0.6750\n",
            "Epoch 38/200\n",
            "24/24 - 0s - loss: 0.5722 - accuracy: 0.7000\n",
            "Epoch 39/200\n",
            "24/24 - 0s - loss: 0.5715 - accuracy: 0.6833\n",
            "Epoch 40/200\n",
            "24/24 - 0s - loss: 0.5633 - accuracy: 0.7083\n",
            "Epoch 41/200\n",
            "24/24 - 0s - loss: 0.5572 - accuracy: 0.6833\n",
            "Epoch 42/200\n",
            "24/24 - 0s - loss: 0.5617 - accuracy: 0.6917\n",
            "Epoch 43/200\n",
            "24/24 - 0s - loss: 0.5539 - accuracy: 0.7083\n",
            "Epoch 44/200\n",
            "24/24 - 0s - loss: 0.5460 - accuracy: 0.7000\n",
            "Epoch 45/200\n",
            "24/24 - 0s - loss: 0.5434 - accuracy: 0.7250\n",
            "Epoch 46/200\n",
            "24/24 - 0s - loss: 0.5381 - accuracy: 0.7000\n",
            "Epoch 47/200\n",
            "24/24 - 0s - loss: 0.5388 - accuracy: 0.6917\n",
            "Epoch 48/200\n",
            "24/24 - 0s - loss: 0.5355 - accuracy: 0.7000\n",
            "Epoch 49/200\n",
            "24/24 - 0s - loss: 0.5381 - accuracy: 0.6917\n",
            "Epoch 50/200\n",
            "24/24 - 0s - loss: 0.5314 - accuracy: 0.7083\n",
            "Epoch 51/200\n",
            "24/24 - 0s - loss: 0.5254 - accuracy: 0.7250\n",
            "Epoch 52/200\n",
            "24/24 - 0s - loss: 0.5222 - accuracy: 0.7083\n",
            "Epoch 53/200\n",
            "24/24 - 0s - loss: 0.5245 - accuracy: 0.7250\n",
            "Epoch 54/200\n",
            "24/24 - 0s - loss: 0.5174 - accuracy: 0.7250\n",
            "Epoch 55/200\n",
            "24/24 - 0s - loss: 0.5184 - accuracy: 0.7167\n",
            "Epoch 56/200\n",
            "24/24 - 0s - loss: 0.5168 - accuracy: 0.7250\n",
            "Epoch 57/200\n",
            "24/24 - 0s - loss: 0.5151 - accuracy: 0.6833\n",
            "Epoch 58/200\n",
            "24/24 - 0s - loss: 0.5094 - accuracy: 0.7167\n",
            "Epoch 59/200\n",
            "24/24 - 0s - loss: 0.5085 - accuracy: 0.7000\n",
            "Epoch 60/200\n",
            "24/24 - 0s - loss: 0.5093 - accuracy: 0.7167\n",
            "Epoch 61/200\n",
            "24/24 - 0s - loss: 0.5074 - accuracy: 0.7000\n",
            "Epoch 62/200\n",
            "24/24 - 0s - loss: 0.5029 - accuracy: 0.7000\n",
            "Epoch 63/200\n",
            "24/24 - 0s - loss: 0.5009 - accuracy: 0.7083\n",
            "Epoch 64/200\n",
            "24/24 - 0s - loss: 0.5002 - accuracy: 0.7083\n",
            "Epoch 65/200\n",
            "24/24 - 0s - loss: 0.4969 - accuracy: 0.7250\n",
            "Epoch 66/200\n",
            "24/24 - 0s - loss: 0.4974 - accuracy: 0.7083\n",
            "Epoch 67/200\n",
            "24/24 - 0s - loss: 0.4931 - accuracy: 0.7167\n",
            "Epoch 68/200\n",
            "24/24 - 0s - loss: 0.4939 - accuracy: 0.7167\n",
            "Epoch 69/200\n",
            "24/24 - 0s - loss: 0.4920 - accuracy: 0.7250\n",
            "Epoch 70/200\n",
            "24/24 - 0s - loss: 0.4923 - accuracy: 0.7250\n",
            "Epoch 71/200\n",
            "24/24 - 0s - loss: 0.4907 - accuracy: 0.7333\n",
            "Epoch 72/200\n",
            "24/24 - 0s - loss: 0.4939 - accuracy: 0.6917\n",
            "Epoch 73/200\n",
            "24/24 - 0s - loss: 0.4875 - accuracy: 0.7083\n",
            "Epoch 74/200\n",
            "24/24 - 0s - loss: 0.4852 - accuracy: 0.7083\n",
            "Epoch 75/200\n",
            "24/24 - 0s - loss: 0.4869 - accuracy: 0.7250\n",
            "Epoch 76/200\n",
            "24/24 - 0s - loss: 0.4848 - accuracy: 0.6917\n",
            "Epoch 77/200\n",
            "24/24 - 0s - loss: 0.4840 - accuracy: 0.7250\n",
            "Epoch 78/200\n",
            "24/24 - 0s - loss: 0.4849 - accuracy: 0.7250\n",
            "Epoch 79/200\n",
            "24/24 - 0s - loss: 0.4844 - accuracy: 0.7083\n",
            "Epoch 80/200\n",
            "24/24 - 0s - loss: 0.4797 - accuracy: 0.7167\n",
            "Epoch 81/200\n",
            "24/24 - 0s - loss: 0.4861 - accuracy: 0.7000\n",
            "Epoch 82/200\n",
            "24/24 - 0s - loss: 0.4824 - accuracy: 0.7333\n",
            "Epoch 83/200\n",
            "24/24 - 0s - loss: 0.4808 - accuracy: 0.7417\n",
            "Epoch 84/200\n",
            "24/24 - 0s - loss: 0.4809 - accuracy: 0.7500\n",
            "Epoch 85/200\n",
            "24/24 - 0s - loss: 0.4751 - accuracy: 0.7167\n",
            "Epoch 86/200\n",
            "24/24 - 0s - loss: 0.4758 - accuracy: 0.7083\n",
            "Epoch 87/200\n",
            "24/24 - 0s - loss: 0.4732 - accuracy: 0.7083\n",
            "Epoch 88/200\n",
            "24/24 - 0s - loss: 0.4754 - accuracy: 0.7167\n",
            "Epoch 89/200\n",
            "24/24 - 0s - loss: 0.4754 - accuracy: 0.7250\n",
            "Epoch 90/200\n",
            "24/24 - 0s - loss: 0.4749 - accuracy: 0.7167\n",
            "Epoch 91/200\n",
            "24/24 - 0s - loss: 0.4701 - accuracy: 0.7417\n",
            "Epoch 92/200\n",
            "24/24 - 0s - loss: 0.4729 - accuracy: 0.7417\n",
            "Epoch 93/200\n",
            "24/24 - 0s - loss: 0.4718 - accuracy: 0.7167\n",
            "Epoch 94/200\n",
            "24/24 - 0s - loss: 0.4697 - accuracy: 0.7250\n",
            "Epoch 95/200\n",
            "24/24 - 0s - loss: 0.4682 - accuracy: 0.7417\n",
            "Epoch 96/200\n",
            "24/24 - 0s - loss: 0.4685 - accuracy: 0.7333\n",
            "Epoch 97/200\n",
            "24/24 - 0s - loss: 0.4684 - accuracy: 0.7333\n",
            "Epoch 98/200\n",
            "24/24 - 0s - loss: 0.4653 - accuracy: 0.7500\n",
            "Epoch 99/200\n",
            "24/24 - 0s - loss: 0.4676 - accuracy: 0.7333\n",
            "Epoch 100/200\n",
            "24/24 - 0s - loss: 0.4684 - accuracy: 0.7333\n",
            "Epoch 101/200\n",
            "24/24 - 0s - loss: 0.4680 - accuracy: 0.7250\n",
            "Epoch 102/200\n",
            "24/24 - 0s - loss: 0.4645 - accuracy: 0.7333\n",
            "Epoch 103/200\n",
            "24/24 - 0s - loss: 0.4648 - accuracy: 0.7583\n",
            "Epoch 104/200\n",
            "24/24 - 0s - loss: 0.4669 - accuracy: 0.7083\n",
            "Epoch 105/200\n",
            "24/24 - 0s - loss: 0.4682 - accuracy: 0.7333\n",
            "Epoch 106/200\n",
            "24/24 - 0s - loss: 0.4640 - accuracy: 0.7583\n",
            "Epoch 107/200\n",
            "24/24 - 0s - loss: 0.4612 - accuracy: 0.7333\n",
            "Epoch 108/200\n",
            "24/24 - 0s - loss: 0.4662 - accuracy: 0.7167\n",
            "Epoch 109/200\n",
            "24/24 - 0s - loss: 0.4665 - accuracy: 0.7083\n",
            "Epoch 110/200\n",
            "24/24 - 0s - loss: 0.4619 - accuracy: 0.7167\n",
            "Epoch 111/200\n",
            "24/24 - 0s - loss: 0.4616 - accuracy: 0.7417\n",
            "Epoch 112/200\n",
            "24/24 - 0s - loss: 0.4604 - accuracy: 0.7250\n",
            "Epoch 113/200\n",
            "24/24 - 0s - loss: 0.4581 - accuracy: 0.7333\n",
            "Epoch 114/200\n",
            "24/24 - 0s - loss: 0.4613 - accuracy: 0.7333\n",
            "Epoch 115/200\n",
            "24/24 - 0s - loss: 0.4603 - accuracy: 0.7250\n",
            "Epoch 116/200\n",
            "24/24 - 0s - loss: 0.4733 - accuracy: 0.7167\n",
            "Epoch 117/200\n",
            "24/24 - 0s - loss: 0.4661 - accuracy: 0.7250\n",
            "Epoch 118/200\n",
            "24/24 - 0s - loss: 0.4550 - accuracy: 0.7417\n",
            "Epoch 119/200\n",
            "24/24 - 0s - loss: 0.4582 - accuracy: 0.7250\n",
            "Epoch 120/200\n",
            "24/24 - 0s - loss: 0.4587 - accuracy: 0.7417\n",
            "Epoch 121/200\n",
            "24/24 - 0s - loss: 0.4591 - accuracy: 0.7250\n",
            "Epoch 122/200\n",
            "24/24 - 0s - loss: 0.4542 - accuracy: 0.7417\n",
            "Epoch 123/200\n",
            "24/24 - 0s - loss: 0.4573 - accuracy: 0.7250\n",
            "Epoch 124/200\n",
            "24/24 - 0s - loss: 0.4525 - accuracy: 0.7500\n",
            "Epoch 125/200\n",
            "24/24 - 0s - loss: 0.4561 - accuracy: 0.7500\n",
            "Epoch 126/200\n",
            "24/24 - 0s - loss: 0.4532 - accuracy: 0.7500\n",
            "Epoch 127/200\n",
            "24/24 - 0s - loss: 0.4561 - accuracy: 0.7417\n",
            "Epoch 128/200\n",
            "24/24 - 0s - loss: 0.4563 - accuracy: 0.7500\n",
            "Epoch 129/200\n",
            "24/24 - 0s - loss: 0.4518 - accuracy: 0.7417\n",
            "Epoch 130/200\n",
            "24/24 - 0s - loss: 0.4536 - accuracy: 0.7500\n",
            "Epoch 131/200\n",
            "24/24 - 0s - loss: 0.4534 - accuracy: 0.7583\n",
            "Epoch 132/200\n",
            "24/24 - 0s - loss: 0.4554 - accuracy: 0.7583\n",
            "Epoch 133/200\n",
            "24/24 - 0s - loss: 0.4513 - accuracy: 0.7333\n",
            "Epoch 134/200\n",
            "24/24 - 0s - loss: 0.4512 - accuracy: 0.7333\n",
            "Epoch 135/200\n",
            "24/24 - 0s - loss: 0.4504 - accuracy: 0.7583\n",
            "Epoch 136/200\n",
            "24/24 - 0s - loss: 0.4503 - accuracy: 0.7417\n",
            "Epoch 137/200\n",
            "24/24 - 0s - loss: 0.4490 - accuracy: 0.7500\n",
            "Epoch 138/200\n",
            "24/24 - 0s - loss: 0.4500 - accuracy: 0.7417\n",
            "Epoch 139/200\n",
            "24/24 - 0s - loss: 0.4521 - accuracy: 0.7667\n",
            "Epoch 140/200\n",
            "24/24 - 0s - loss: 0.4492 - accuracy: 0.7250\n",
            "Epoch 141/200\n",
            "24/24 - 0s - loss: 0.4515 - accuracy: 0.7500\n",
            "Epoch 142/200\n",
            "24/24 - 0s - loss: 0.4525 - accuracy: 0.7333\n",
            "Epoch 143/200\n",
            "24/24 - 0s - loss: 0.4466 - accuracy: 0.7500\n",
            "Epoch 144/200\n",
            "24/24 - 0s - loss: 0.4497 - accuracy: 0.7417\n",
            "Epoch 145/200\n",
            "24/24 - 0s - loss: 0.4557 - accuracy: 0.7500\n",
            "Epoch 146/200\n",
            "24/24 - 0s - loss: 0.4487 - accuracy: 0.7500\n",
            "Epoch 147/200\n",
            "24/24 - 0s - loss: 0.4486 - accuracy: 0.7583\n",
            "Epoch 148/200\n",
            "24/24 - 0s - loss: 0.4481 - accuracy: 0.7417\n",
            "Epoch 149/200\n",
            "24/24 - 0s - loss: 0.4450 - accuracy: 0.7417\n",
            "Epoch 150/200\n",
            "24/24 - 0s - loss: 0.4485 - accuracy: 0.7833\n",
            "Epoch 151/200\n",
            "24/24 - 0s - loss: 0.4445 - accuracy: 0.7500\n",
            "Epoch 152/200\n",
            "24/24 - 0s - loss: 0.4457 - accuracy: 0.7583\n",
            "Epoch 153/200\n",
            "24/24 - 0s - loss: 0.4444 - accuracy: 0.7333\n",
            "Epoch 154/200\n",
            "24/24 - 0s - loss: 0.4462 - accuracy: 0.7250\n",
            "Epoch 155/200\n",
            "24/24 - 0s - loss: 0.4493 - accuracy: 0.7500\n",
            "Epoch 156/200\n",
            "24/24 - 0s - loss: 0.4416 - accuracy: 0.7583\n",
            "Epoch 157/200\n",
            "24/24 - 0s - loss: 0.4438 - accuracy: 0.7583\n",
            "Epoch 158/200\n",
            "24/24 - 0s - loss: 0.4485 - accuracy: 0.7083\n",
            "Epoch 159/200\n",
            "24/24 - 0s - loss: 0.4441 - accuracy: 0.7583\n",
            "Epoch 160/200\n",
            "24/24 - 0s - loss: 0.4405 - accuracy: 0.7583\n",
            "Epoch 161/200\n",
            "24/24 - 0s - loss: 0.4423 - accuracy: 0.7500\n",
            "Epoch 162/200\n",
            "24/24 - 0s - loss: 0.4410 - accuracy: 0.7500\n",
            "Epoch 163/200\n",
            "24/24 - 0s - loss: 0.4393 - accuracy: 0.7667\n",
            "Epoch 164/200\n",
            "24/24 - 0s - loss: 0.4438 - accuracy: 0.7417\n",
            "Epoch 165/200\n",
            "24/24 - 0s - loss: 0.4466 - accuracy: 0.7750\n",
            "Epoch 166/200\n",
            "24/24 - 0s - loss: 0.4392 - accuracy: 0.7500\n",
            "Epoch 167/200\n",
            "24/24 - 0s - loss: 0.4385 - accuracy: 0.7583\n",
            "Epoch 168/200\n",
            "24/24 - 0s - loss: 0.4394 - accuracy: 0.7750\n",
            "Epoch 169/200\n",
            "24/24 - 0s - loss: 0.4424 - accuracy: 0.7833\n",
            "Epoch 170/200\n",
            "24/24 - 0s - loss: 0.4423 - accuracy: 0.7250\n",
            "Epoch 171/200\n",
            "24/24 - 0s - loss: 0.4379 - accuracy: 0.7583\n",
            "Epoch 172/200\n",
            "24/24 - 0s - loss: 0.4438 - accuracy: 0.7583\n",
            "Epoch 173/200\n",
            "24/24 - 0s - loss: 0.4400 - accuracy: 0.7667\n",
            "Epoch 174/200\n",
            "24/24 - 0s - loss: 0.4386 - accuracy: 0.7500\n",
            "Epoch 175/200\n",
            "24/24 - 0s - loss: 0.4406 - accuracy: 0.7333\n",
            "Epoch 176/200\n",
            "24/24 - 0s - loss: 0.4387 - accuracy: 0.7333\n",
            "Epoch 177/200\n",
            "24/24 - 0s - loss: 0.4349 - accuracy: 0.7667\n",
            "Epoch 178/200\n",
            "24/24 - 0s - loss: 0.4373 - accuracy: 0.7667\n",
            "Epoch 179/200\n",
            "24/24 - 0s - loss: 0.4353 - accuracy: 0.7833\n",
            "Epoch 180/200\n",
            "24/24 - 0s - loss: 0.4385 - accuracy: 0.7667\n",
            "Epoch 181/200\n",
            "24/24 - 0s - loss: 0.4345 - accuracy: 0.7750\n",
            "Epoch 182/200\n",
            "24/24 - 0s - loss: 0.4346 - accuracy: 0.7750\n",
            "Epoch 183/200\n",
            "24/24 - 0s - loss: 0.4338 - accuracy: 0.7750\n",
            "Epoch 184/200\n",
            "24/24 - 0s - loss: 0.4349 - accuracy: 0.7750\n",
            "Epoch 185/200\n",
            "24/24 - 0s - loss: 0.4363 - accuracy: 0.7583\n",
            "Epoch 186/200\n",
            "24/24 - 0s - loss: 0.4361 - accuracy: 0.7667\n",
            "Epoch 187/200\n",
            "24/24 - 0s - loss: 0.4372 - accuracy: 0.7667\n",
            "Epoch 188/200\n",
            "24/24 - 0s - loss: 0.4371 - accuracy: 0.7667\n",
            "Epoch 189/200\n",
            "24/24 - 0s - loss: 0.4333 - accuracy: 0.7583\n",
            "Epoch 190/200\n",
            "24/24 - 0s - loss: 0.4321 - accuracy: 0.7583\n",
            "Epoch 191/200\n",
            "24/24 - 0s - loss: 0.4352 - accuracy: 0.7667\n",
            "Epoch 192/200\n",
            "24/24 - 0s - loss: 0.4320 - accuracy: 0.7583\n",
            "Epoch 193/200\n",
            "24/24 - 0s - loss: 0.4332 - accuracy: 0.7500\n",
            "Epoch 194/200\n",
            "24/24 - 0s - loss: 0.4355 - accuracy: 0.7500\n",
            "Epoch 195/200\n",
            "24/24 - 0s - loss: 0.4321 - accuracy: 0.7583\n",
            "Epoch 196/200\n",
            "24/24 - 0s - loss: 0.4292 - accuracy: 0.7750\n",
            "Epoch 197/200\n",
            "24/24 - 0s - loss: 0.4336 - accuracy: 0.7750\n",
            "Epoch 198/200\n",
            "24/24 - 0s - loss: 0.4299 - accuracy: 0.7833\n",
            "Epoch 199/200\n",
            "24/24 - 0s - loss: 0.4351 - accuracy: 0.7667\n",
            "Epoch 200/200\n",
            "24/24 - 0s - loss: 0.4294 - accuracy: 0.7833\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.5008 - accuracy: 0.7667\n",
            "Final test set loss: 0.500758\n",
            "Final test set accuracy: 0.766667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}