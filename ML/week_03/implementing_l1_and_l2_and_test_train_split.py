# -*- coding: utf-8 -*-
"""Implementing l1 and l2 and test_train_split.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KGhxTw8afoK_6jcm81nVihTiEbYF1dA
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso



# Load the boston dataset
boston_X, boston_y = datasets.load_boston(return_X_y=True)

# Use only one feature
boston_X = boston_X[:, np.newaxis, 5]

# Split the data into training/testing sets
boston_X_train, boston_X_test = train_test_split(boston_X, test_size=0.2, random_state=42)
# boston_X_train = boston_X[:-50]
# boston_X_test = boston_X[-50:]

# Split the targets into training/testing sets
boston_y_train, boston_y_test = train_test_split(boston_y, test_size=0.2, random_state=42) 
# boston_y_train = boston_y[:-50]
# boston_y_test = boston_y[-50:]

# Create linear regression object
#regr = linear_model.LinearRegression()

# create lasso
lasso_reg = Lasso(alpha=0.1)

# Train the model using the training sets
lasso_reg.fit(boston_X_train, boston_y_train)

# Make predictions using the testing set
boston_y_pred = lasso_reg.predict(boston_X_test)

# The coefficients
#print('Coefficients: \n', regr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(boston_y_test, boston_y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(boston_y_test, boston_y_pred))

# Plot outputs
plt.scatter(boston_X_test, boston_y_test,  color='black')
plt.plot(boston_X_test, boston_y_pred, color='blue', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()

import numpy as np

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import tensorflow as tf
from tensorflow import keras


iris_data = load_iris() # load the iris dataset

print('Example data: ')
print(iris_data.data[:5])
print('Example labels: ')
print(iris_data.target[:5])

x = iris_data.data[:, :2] 
y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column
y_ = np.where(y_==2, 1, y_)

# One Hot encode the class labels
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y_)
print('all of y')
print(y)

# Split the data for training and testing
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)

# Build the model

# model = Sequential()

model = keras.models.Sequential([
  keras.layers.Dense(10, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(l=0.1)),
  keras.layers.Dense(10, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(l=0.1)),
  keras.layers.Dense(2, activation=tf.nn.sigmoid)
])

model.add(Dense(10, activation='relu', name='fc1'))  # input_shape=(4,)
model.add(Dense(10, activation='relu', name='fc2'))
model.add(Dense(2, activation='softmax', name='output'))

# Adam optimizer with learning rate of 0.001
optimizer = Adam(lr=0.001)
model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

print('Neural Network Model Summary: ')
#print(model.summary())


# Train the model
model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)

# Test on unseen data

results = model.evaluate(test_x, test_y)